{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d519a567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/seancafferty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from emoji import UNICODE_EMOJI\n",
    "import demoji\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "database = 'postgres'\n",
    "user = 'postgres'\n",
    "password = 'rKKFiDXpiu6Wbv3'\n",
    "host='47.200.121.209'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a783bc",
   "metadata": {},
   "source": [
    "### Preprocessing - PCA of TFIDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011b9a7",
   "metadata": {},
   "source": [
    "### Get the handles for preprocessing. In our case, we have them stored in a larger dataframe of information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f317780",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_csv('base_info_handles.csv')\n",
    "base['HANDLE'] = base['HANDLE'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf57169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d71588",
   "metadata": {},
   "outputs": [],
   "source": [
    "base['HANDLE'] = base['HANDLE'].fillna(0)\n",
    "list_of_rus_handles = [handle[1:] for handle in list(base[(base['Source']=='RUS') & (base['HANDLE']!=0)]['HANDLE'].values)]\n",
    "list_of_usa_handles = [handle[1:] for handle in list(base[(base['Source']=='USA') & (base['HANDLE']!=0)]['HANDLE'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "460b95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_dictionary = dict(zip(list(base.HANDLE),list(base.Country)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bfb6f9",
   "metadata": {},
   "source": [
    "### Get user activity from the Tweets database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2b1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_user_activity(user_name):\n",
    "    from sqlalchemy import create_engine\n",
    "    import pandas as pd\n",
    "    engine = create_engine(f'postgresql://{user}:{password}@{host}:5432/{user}')\n",
    "    user_id =  pd.read_sql_query(f\"SELECT user_id FROM twitter_profiles WHERE screen_name = ('{user_name}')\",con=engine)['user_id'][0]\n",
    "    table_data  =  pd.read_sql_query(f\"SELECT * FROM tweets WHERE (tweet_id IN (SELECT tweet_id FROM retweets WHERE user_id = ('{user_id}'))) OR (user_id = ('{user_id}'))\",con=engine)\n",
    "    return table_data\n",
    "\n",
    "def get_network_df(embassy_list):\n",
    "    embassy_df = get_all_user_activity(embassy_list[0])\n",
    "    embassy_df['Country'] = handle_dictionary['@'+embassy_list[0]]\n",
    "    for handle in tqdm(embassy_list[1:]):\n",
    "        print(f'...getting data from {handle}')\n",
    "        df2 = get_all_user_activity(handle)\n",
    "        df2['Country'] = handle_dictionary['@'+handle]\n",
    "        embassy_df = pd.concat([embassy_df,df2])\n",
    "    return embassy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53253e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_network_df(list_of_usa_handles)\n",
    "# df.to_csv('all_usa_tweets')\n",
    "# df2 = get_network_df(list_of_rus_handles)\n",
    "# df2.to_csv('all_rus_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b47552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff = get_all_user_activity('StateDept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1315d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usa = pd.read_csv('all_usa_tweets.csv')\n",
    "#rus = pd.read_csv('all_rus_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d305ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(usa[usa['user_id']==9624742])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb532cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get only English-language Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8247902",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_documents_usa = usa[usa.lang=='en'][['clean_text','Country']]\n",
    "english_documents_usa_df = pd.DataFrame(english_documents_usa).rename(columns={'clean_text':'documents'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4219b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_documents_rus = rus[rus.lang=='en'][['clean_text','Country']]\n",
    "english_documents_rus_df = pd.DataFrame(english_documents_rus).rename(columns={'clean_text':'documents'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac7dbaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4483cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(document_df,stopword_lang):\n",
    "    lines = document_df['documents'].fillna(0).values.tolist()\n",
    "    sentences = list()\n",
    "    for line in tqdm(lines):\n",
    "        tokens = word_tokenize(line) if line!=0 else 'empty'\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        table = str.maketrans('','',string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(stopword_lang))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        sentences.append(words)\n",
    "    document_df['preprocessed_text'] = sentences\n",
    "    return document_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9273588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311537/311537 [02:16<00:00, 2287.77it/s]\n"
     ]
    }
   ],
   "source": [
    "english_preprocessed_us = preprocess_text(english_documents_usa_df,'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e692e748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156613/156613 [01:10<00:00, 2215.92it/s]\n"
     ]
    }
   ],
   "source": [
    "english_preprocessed_rus = preprocess_text(english_documents_rus_df,'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae1ffee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the tweets into TFIDF-friendly list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e5d4040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_documents(df):\n",
    "    countries = list(df.Country.unique())\n",
    "    documents = []\n",
    "    for country in tqdm(countries):\n",
    "        new_list = []\n",
    "        for entry in df[df['Country']==country].preprocessed_text.values:\n",
    "            new_list += entry\n",
    "        \" \".join(new_list)\n",
    "        documents.append(\" \".join(new_list))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8192e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [00:02<00:00, 54.73it/s]\n",
      "100%|██████████| 130/130 [00:01<00:00, 101.15it/s]\n"
     ]
    }
   ],
   "source": [
    "documents_us = make_documents(english_preprocessed_us)\n",
    "documents_rus = make_documents(english_preprocessed_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba27ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = documents_us + documents_rus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dd2f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pca_tfidf(all_documents):\n",
    "    tfidfvectoriser=TfidfVectorizer(ngram_range=(1,2), max_features=10000, min_df=10)\n",
    "    X=tfidfvectoriser.fit_transform(all_documents).todense()\n",
    "    countries_us = list(english_preprocessed_us.Country.unique())\n",
    "    countries_rus = list(english_preprocessed_rus.Country.unique())\n",
    "    countries_us_class = ['America'] * len(countries_us)\n",
    "    countries_rus_class = ['Russia'] * len(countries_rus)\n",
    "    countries_us_binary = ['red'] * len(countries_us)\n",
    "    countries_rus_binary = ['blue'] * len(countries_rus)\n",
    "    continent_name_us = list(base[(base['Source']=='USA') & (base['HANDLE']!=0)]['ContinentName'])\n",
    "    continent_name_rus = list(base[(base['Source']=='RUS') & (base['HANDLE']!=0)]['ContinentName'])\n",
    "    iso_us = list(base[(base['Source']=='USA') & (base['HANDLE']!=0)]['ISO_A3'])\n",
    "    iso_rus = list(base[(base['Source']=='RUS') & (base['HANDLE']!=0)]['ISO_A3'])\n",
    "    countries = countries_us + countries_rus\n",
    "    classes = countries_us_class + countries_rus_class\n",
    "    binary_class = countries_us_binary + countries_rus_binary\n",
    "    continent_name = continent_name_us + continent_name_rus\n",
    "    iso = iso_us + iso_rus\n",
    "    pca2 = PCA(n_components=2).fit(X)\n",
    "    pca3 = PCA(n_components=3).fit(X)\n",
    "    data2D = pca2.transform(X)\n",
    "    data3D = pca3.transform(X)\n",
    "    pca_df2 = pd.DataFrame(data2D, columns = ['2D-0','2D-1'])\n",
    "    pca_df3 = pd.DataFrame(data3D, columns = ['3D-0','3D-1','3D-2'])\n",
    "    pca_df = pd.concat([pca_df2,pca_df3],axis = 1)\n",
    "    pca_df['Country'] = countries\n",
    "    pca_df['class'] = classes\n",
    "    pca_df['binary_class'] = binary_class\n",
    "    pca_df['continent_name'] = continent_name\n",
    "    pca_df['ISO_A3'] = iso\n",
    "    color_dict = {'Asia':'red','South America':'green','Central America':'lightgreen','Australia':'blue','Africa':'orange','North America':'yellow','Europe':'purple'}\n",
    "    pca_df['continent_colors'] = [color_dict[cont] for cont in pca_df['continent_name']]\n",
    "    \n",
    "    return pca_df.to_csv('tfidf_pca_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b3ae4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pca_tfidf(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "415acaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca_df.to_csv('tfidf_pca_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "224ef5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_pca_df = pd.read_csv('tfidf_pca_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "66539675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9037b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bert Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a58dd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert = pd.read_csv('BERT_sentence_averages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1a0a062a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "634b74a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 771)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1891632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pca_bert(df):\n",
    "    X = df[list(df.columns[1:-2])]\n",
    "    countries_us = list(english_preprocessed_us.Country.unique())\n",
    "    countries_rus = list(english_preprocessed_rus.Country.unique())\n",
    "    countries_us_class = ['America'] * len(countries_us)\n",
    "    countries_rus_class = ['Russia'] * len(countries_rus)\n",
    "    countries_us_binary = ['red'] * len(countries_us)\n",
    "    countries_rus_binary = ['blue'] * len(countries_rus)\n",
    "    continent_name_us = list(base[(base['Source']=='USA') & (base['HANDLE']!=0)]['ContinentName'])\n",
    "    continent_name_rus = list(base[(base['Source']=='RUS') & (base['HANDLE']!=0)]['ContinentName'])\n",
    "    iso_us = list(base[(base['Source']=='USA') & (base['HANDLE']!=0)]['ISO_A3'])\n",
    "    iso_rus = list(base[(base['Source']=='RUS') & (base['HANDLE']!=0)]['ISO_A3'])\n",
    "    countries = countries_us + countries_rus\n",
    "    classes = countries_us_class + countries_rus_class\n",
    "    binary_class = countries_us_binary + countries_rus_binary\n",
    "    continent_name = continent_name_us + continent_name_rus\n",
    "    iso = iso_us + iso_rus\n",
    "    pca2 = PCA(n_components=2).fit(X)\n",
    "    pca3 = PCA(n_components=3).fit(X)\n",
    "    data2D = pca2.transform(X)\n",
    "    data3D = pca3.transform(X)\n",
    "    pca_df2 = pd.DataFrame(data2D, columns = ['2D-0','2D-1'])\n",
    "    pca_df3 = pd.DataFrame(data3D, columns = ['3D-0','3D-1','3D-2'])\n",
    "    pca_df = pd.concat([pca_df2,pca_df3],axis = 1)\n",
    "    pca_df['Country'] = countries\n",
    "    pca_df['class'] = classes\n",
    "    pca_df['binary_class'] = binary_class\n",
    "    pca_df['continent_name'] = continent_name\n",
    "    pca_df['ISO_A3'] = iso\n",
    "    color_dict = {'Asia':'red','South America':'green','Central America':'lightgreen','Australia':'blue','Africa':'orange','North America':'yellow','Europe':'purple'}\n",
    "    pca_df['continent_colors'] = [color_dict[cont] for cont in pca_df['continent_name']]\n",
    "    \n",
    "    return pca_df.to_csv('bert_pca_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "910f0b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pca_bert(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "064f3cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pca_df = pd.read_csv('bert_pca_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2ca3c4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', '2D-0', '2D-1', '3D-0', '3D-1', '3D-2', 'Country',\n",
       "       'class', 'binary_class', 'continent_name', 'ISO_A3',\n",
       "       'continent_colors'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pca_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb643c",
   "metadata": {},
   "source": [
    "## Test Visualizations of Semantic Vector Space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "5c9d532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot\n",
    "\n",
    "\n",
    "def update_clusters(color_scheme='network',data='BERT',n_clusters=3,dimension='2D',show_k=True, text=True):\n",
    "    if data == 'BERT':\n",
    "        dff = bert_pca_df\n",
    "    elif data == 'TFIDF':\n",
    "        dff = tfidf_pca_df\n",
    "\n",
    "    if text == True:\n",
    "        marker_text = 'markers+text'\n",
    "    else:\n",
    "        marker_text = 'markers'\n",
    "        \n",
    "    if color_scheme == 'network':\n",
    "        color_variable = dff['binary_class']\n",
    "    elif color_scheme == 'continent':\n",
    "        color_variable = dff['continent_colors']\n",
    "    \n",
    "    if dimension == '2D':\n",
    "        trace_1 = go.Scatter(\n",
    "            x=dff['2D-0'],\n",
    "            y=dff['2D-1'],\n",
    "            text=dff['Country'],\n",
    "            textposition = 'top center',\n",
    "            mode=marker_text,\n",
    "            marker=dict(\n",
    "                size=16,\n",
    "                color=color_variable,                # set color to an array/list of desired values\n",
    "                colorscale='spectral',   # choose a colorscale\n",
    "                opacity=0.6,\n",
    "                line=dict(\n",
    "                        color='MediumPurple',\n",
    "                        width=1\n",
    "                    )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if show_k == True:\n",
    "            km = KMeans(n_clusters)\n",
    "            clusts = km.fit_predict(np.array(pca_df[['2D-0','2D-1']]))\n",
    "            centers = pd.DataFrame(km.cluster_centers_)#.transpose()\n",
    "\n",
    "\n",
    "            trace_2 = go.Scatter(\n",
    "                x = centers[0],\n",
    "                y = centers[1],\n",
    "                mode='markers+text',\n",
    "                marker=dict(\n",
    "                    size=123,\n",
    "                    color='green',               \n",
    "                    opacity=0.3\n",
    "                )\n",
    "            )\n",
    "            fig = go.Figure(data=[trace_2,trace_1])\n",
    "        else:\n",
    "            fig = go.Figure(data=[trace_1]) \n",
    "            \n",
    "    elif dimension == '3D':\n",
    "        trace_1 = go.Scatter3d(\n",
    "            x=dff['3D-0'],\n",
    "            y=dff['3D-1'],\n",
    "            z=dff['3D-2'],\n",
    "            text=dff['Country'],\n",
    "            textposition = 'top center',\n",
    "            mode=marker_text,\n",
    "            marker=dict(\n",
    "                size=6,\n",
    "                color=color_variable,                # set color to an array/list of desired values\n",
    "                colorscale='spectral',   # choose a colorscale\n",
    "                opacity=0.6,\n",
    "                line=dict(\n",
    "                        color='MediumPurple',\n",
    "                        width=1\n",
    "                    )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if show_k == True:\n",
    "            km = KMeans(n_clusters)\n",
    "            clusts = km.fit_predict(np.array(pca_df[['3D-0','3D-1','3D-2']]))\n",
    "            centers = pd.DataFrame(km.cluster_centers_)#.transpose()\n",
    "\n",
    "\n",
    "            trace_2 = go.Scatter3d(\n",
    "                x = centers[0],\n",
    "                y = centers[1],\n",
    "                z = centers[2],\n",
    "                mode='markers+text',\n",
    "                marker=dict(\n",
    "                    size=123,\n",
    "                    color='green',               \n",
    "                    opacity=0.3\n",
    "                )\n",
    "            )\n",
    "            fig = go.Figure(data=[trace_2,trace_1])\n",
    "        else:\n",
    "            fig = go.Figure(data=[trace_1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig.update_traces(textposition='top center')\n",
    "    fig.update_layout(\n",
    "            height=800,\n",
    "            #title_text='TFIDF-Tweet Accounts'\n",
    "        )    \n",
    "    camera = dict(\n",
    "        eye=dict(x=2, y=2, z=0.1),\n",
    "        center=dict(x=0.5, y=0.7, z=0),\n",
    "        up=dict(x=0, y=0, z=1)\n",
    "    )\n",
    "\n",
    "    fig.update_layout(scene_camera=camera)\n",
    "    \n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ac3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update_clusters(color_scheme='continent',data='BERT',n_clusters=1,dimension='2D',show_k=False, text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2cf132f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(bert.country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "d20d7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_messaging(df,country,network):\n",
    "    country_array = np.array(df[(df.country==country)&(df.network==network)][list(df.columns)[1:-2]].iloc[0]).reshape(1,-1)\n",
    "    X = df[list(df.columns)[1:-2]]\n",
    "    sims  = []\n",
    "    for idx in range(len(X)):\n",
    "        sim = cosine_similarity(country_array, np.array(X.iloc[idx]).reshape(1,-1))\n",
    "        sims.append(sim[0][0])\n",
    "    top_df = pd.DataFrame(sims,columns=['cosine_similarity'])\n",
    "    top_df['country'] = df['country']\n",
    "    top_df['network'] = df['network']\n",
    "    top_df = top_df.sort_values(by='cosine_similarity',ascending=False)\n",
    "    top_df = top_df[['country','network','cosine_similarity']]\n",
    "    return top_df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "228f87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_most_similar(bert,'YEMEN','RUS').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "22e1533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_different(df):\n",
    "    countries = set(list(df[df.network=='USA'].country)).intersection(set(list(df[df.network=='RUS'].country)))\n",
    "    sims = []\n",
    "    for country in countries:\n",
    "        country_array_rus = np.array(df[(df.country==country)&(df.network=='RUS')][list(df.columns)[1:-2]].iloc[0]).reshape(1,-1)\n",
    "        country_array_usa = np.array(df[(df.country==country)&(df.network=='USA')][list(df.columns)[1:-2]].iloc[0]).reshape(1,-1)\n",
    "        sim = cosine_similarity(country_array_rus, country_array_usa)\n",
    "        sims.append(sim[0][0])\n",
    "    top_df = pd.DataFrame(sims,columns=['cosine_similarity'])\n",
    "    top_df['country'] = countries\n",
    "    top_df = top_df.sort_values(by='cosine_similarity',ascending=True)\n",
    "    top_df = top_df[['country','cosine_similarity']]\n",
    "    return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f2986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_most_different(bert)#.to_csv('BERT_divergence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c5f4eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_most_similar(bert,'SAUDI ARABIA','USA').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5bca7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "cc5082f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis(x=None, data=None, cov=None):\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n",
    "    x    : vector or matrix of data with, say, p columns.\n",
    "    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
    "    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - np.mean(data)\n",
    "    if not cov:\n",
    "        cov = np.cov(data.values.T)\n",
    "    inv_covmat = sp.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "import scipy as sp\n",
    "countries = ['afghanistan', 'albania', 'algeria', 'angola', 'argentina', 'armenia', 'australia', 'austria', 'azerbaijan', 'bahamas', 'bahrain', 'bangladesh', 'barbados', 'belarus', 'belgium', 'belize', 'benin', 'bolivia', 'bosnia', 'botswana', 'brazil', 'brunei', 'bulgaria', 'burkina', 'burundi', 'cabo', 'cambodia', 'cameroon', 'canada', 'chad', 'chile', 'china', 'colombia', 'comoros', 'congo', 'costa', 'côte', 'croatia', 'cuba', 'cyprus', 'czechia', 'denmark', 'djibouti', 'dominican', 'ecuador', 'egypt', 'elsalvador', 'equatorialguinea', 'eritrea', 'estonia', 'eswatini', 'ethiopia', 'fiji', 'finland', 'france', 'gabon', 'georgia', 'germany', 'ghana', 'greece', 'guatemala', 'guineabissau', 'guinea', 'guyana', 'haiti','vatican','honduras', 'hungary', 'iceland', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', 'northkorea','southkorea', 'kuwait', 'kyrgyzstan', 'lao', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'lithuania', 'luxembourg', 'madagascar', 'malawi', 'malaysia', 'mali', 'malta', 'mauritania', 'mauritius', 'mexico', 'moldova', 'mongolia', 'montenegro', 'morocco', 'mozambique', 'myanmar', 'nepal', 'netherlands', 'newzealand', 'nicaragua', 'niger', 'nigeria', 'norway', 'oman', 'pakistan', 'palau', 'palestine', 'panama', 'paraguay', 'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', 'russia', 'rwanda', 'saudi', 'saudiarabia','senegal', 'serbia', 'seychelles', 'sierra', 'singapore', 'slovakia', 'slovenia', 'somalia', 'spain', 'srilanka', 'sudan', 'suriname', 'sweden', 'switzerland', 'syria', 'tajikistan', 'tanzania', 'thailand', 'togo', 'trinidad', 'tunisia', 'turkey', 'turkmenistan', 'uganda', 'ukraine', 'uae', 'emirates', 'unitedstates', 'uruguay', 'uzbekistan', 'venezuela', 'vietnam', 'yemen', 'zambia', 'zimbabwe']\n",
    "    \n",
    "def get_mahalanobis_outlier_rank_BERT(df,network):\n",
    "    dff = df[df['network']==network]\n",
    "    columns = list(dff.columns)[1:-2]\n",
    "    matrix = np.array(dff[columns])\n",
    "    vector_df = pd.DataFrame(matrix)\n",
    "    vector_df['network'] = network\n",
    "    vector_df.index = dff.country\n",
    "    cols = list(range(768))\n",
    "    df_x = vector_df[cols]\n",
    "    vector_df['mahalanobis'] = mahalanobis(x=df_x, data=vector_df[cols])\n",
    "    \n",
    "    return pd.DataFrame(vector_df[['network','mahalanobis']].sort_values(by='mahalanobis',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "47bd8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_mahalanobis_outlier_rank_BERT(bert,'USA')#.to_csv('BERT_outliers_usa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f57beb",
   "metadata": {},
   "source": [
    "### Make Language Models - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d1581a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We're using the same dataframe of tweets that we used for TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08e38d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "window = 7\n",
    "\n",
    "languages = ['en','fr','es','ru']\n",
    "stopword_lang = ['english','french','spanish','russian']\n",
    "def w2v(df, language, stopword_lang): \n",
    "    df = df[df['lang']==language]\n",
    "    sentences = list()\n",
    "    lines = df['full_text'].values.tolist()\n",
    "    ## remove urls\n",
    "    lines = [re.sub(r\"http\\S+\", \"\", x) for x in lines]\n",
    "    for line in tqdm(lines):\n",
    "        tokens = word_tokenize(line)\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        table = str.maketrans('','',string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(stopword_lang))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        sentences.append(words)\n",
    "    EMBEDDING_DIM = 100\n",
    "    model = gensim.models.Word2Vec(\n",
    "            sentences = sentences,\n",
    "            size = EMBEDDING_DIM,\n",
    "            window = window,\n",
    "            min_count = 1)\n",
    "    words = list(model.wv.vocab)\n",
    "    print(\"Test Vocabulary size: %d\" % len(words))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "38018e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#russian_embassies_w2v_english = w2v(rus, 'en', 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "180c0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#american_embassies_w2v_english = w2v(usa, 'en', 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3e283d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#american_embassies_w2v_english.save(\"us_en_w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1fdbf62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#russian_embassies_w2v_english.save(\"rus_en_w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee0f7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "american_embassies_w2v_english = Word2Vec.load(\"language_models/us_en_w2v.model\")\n",
    "russian_embassies_w2v_english = Word2Vec.load(\"language_models/rus_en_w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e49291bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#american_embassies_w2v_english.wv.most_similar(['dprk'],topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11ecbe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_association_with_countries(word,model):\n",
    "    countries = ['america','usa','afghanistan', 'albania', 'algeria', 'angola', 'argentina', 'armenia', 'australia', 'austria', 'azerbaijan', 'bahamas', 'bahrain', 'bangladesh', 'barbados', 'belarus', 'belgium', 'belize', 'benin', 'bolivia', 'bosnia', 'botswana', 'brazil', 'brunei', 'bulgaria', 'burkina', 'burundi', 'cabo', 'cambodia', 'cameroon', 'canada', 'chad', 'chile', 'china', 'colombia', 'comoros', 'congo', 'costa', 'côte', 'croatia', 'cuba', 'cyprus', 'czechia', 'denmark', 'djibouti', 'dominican', 'ecuador', 'egypt', 'elsalvador', 'equatorialguinea', 'eritrea', 'estonia', 'eswatini', 'ethiopia', 'fiji', 'finland', 'france', 'gabon', 'georgia', 'germany', 'ghana', 'greece', 'guatemala', 'guineabissau', 'guinea', 'guyana', 'haiti', 'holysee','vatican','honduras', 'hungary', 'iceland', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', 'northkorea','southkorea', 'kuwait', 'kyrgyzstan', 'lao', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'lithuania', 'luxembourg', 'madagascar', 'malawi', 'malaysia', 'mali', 'malta', 'mauritania', 'mauritius', 'mexico', 'micronesia', 'moldova', 'mongolia', 'montenegro', 'morocco', 'mozambique', 'myanmar', 'nepal', 'netherlands', 'newzealand', 'nicaragua', 'niger', 'nigeria', 'norway', 'oman', 'pakistan', 'palau', 'palestine', 'panama', 'papua', 'paraguay', 'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', 'russia', 'rwanda', 'samoa', 'saudi', 'saudiarabia','senegal', 'serbia', 'seychelles', 'sierra', 'singapore', 'slovakia', 'slovenia', 'somalia', 'spain', 'srilanka', 'sudan', 'suriname', 'sweden', 'switzerland', 'syrian', 'tajikistan', 'tanzania', 'thailand', 'timor', 'togo', 'trinidad', 'tunisia', 'turkey', 'turkmenistan', 'uganda', 'ukraine', 'uae', 'unitedarabemirates', 'unitedstates', 'uruguay', 'uzbekistan', 'venezuela', 'viet', 'yemen', 'zambia', 'zimbabwe']\n",
    "    country_dict = {}\n",
    "    for country in countries:\n",
    "        country_dict[country] = model.wv.similarity(word,country)\n",
    "    top_dict = {k: v for k, v in sorted(country_dict.items(), key=lambda item: -item[1])}\n",
    "    return top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7339fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_association_with_countries('dispute',american_embassies_w2v_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e766b",
   "metadata": {},
   "source": [
    "## VISUALIZE WORD2VEC with NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0177dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def word2vec_network(model, word_list, threshhold=0.5):\n",
    "    words, vectors = [], []\n",
    "    for item in word_list:\n",
    "        try:\n",
    "            vectors.append(model.wv.get_vector(item))\n",
    "            words.append(item)\n",
    "        except:\n",
    "            print(f'Word {item} not found in vocab.')\n",
    "    sims = cosine_similarity(vectors, vectors)       \n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(len(vectors)):\n",
    "            if i<=j:\n",
    "                sims[i, j] = False\n",
    "    indices = np.argwhere(sims > threshhold)\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for index in indices:\n",
    "        G.add_edge(words[index[0]], words[index[1]], weight=sims[index[0],\n",
    "                                                                 index[1]])\n",
    "\n",
    "    weight_values = nx.get_edge_attributes(G,'weight')\n",
    "    positions = nx.spring_layout(G)\n",
    "    nx.set_node_attributes(G,name='position',values=positions)\n",
    "    searches = []\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    weights = []\n",
    "    ave_x, ave_y = [], []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = G.nodes[edge[0]]['position']\n",
    "        x1, y1 = G.nodes[edge[1]]['position']\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)\n",
    "        ave_x.append(np.mean([x0, x1]))\n",
    "        ave_y.append(np.mean([y0, y1]))\n",
    "        weights.append(f'{edge[0]}, {edge[1]}: {weight_values[(edge[0], edge[1])]}')\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        opacity=0.3,\n",
    "        line=dict(width=2, color='White'),\n",
    "        hoverinfo=None,\n",
    "        mode='lines')\n",
    "    edge_trace.text = weights\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    sizes = []\n",
    "    for node in G.nodes():\n",
    "        x, y = G.nodes[node]['position']\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        if node in searches:\n",
    "            sizes.append(50)\n",
    "        else:\n",
    "            sizes.append(15)\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers+text',\n",
    "        hoverinfo='text',\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(\n",
    "            showscale=False,\n",
    "            line=dict(color='White'),\n",
    "            colorscale='RdBu',\n",
    "            reversescale=False,\n",
    "            color=[],\n",
    "            opacity=0.9,\n",
    "            size=sizes,\n",
    "            colorbar=dict(\n",
    "                thickness=15,\n",
    "                title='Node Connections',\n",
    "                xanchor='left',\n",
    "                titleside='right'\n",
    "            ),\n",
    "            line_width=2\n",
    "        )\n",
    "    )\n",
    "    invisible_similarity_trace = go.Scatter(\n",
    "        x=ave_x, y=ave_y,\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            color=[],\n",
    "            opacity=0,\n",
    "        )\n",
    "    )\n",
    "    invisible_similarity_trace.text=weights\n",
    "    \n",
    "    node_adjacencies = []\n",
    "    node_text = []\n",
    "    for node, adjacencies in enumerate(G.adjacency()):\n",
    "        node_adjacencies.append(len(adjacencies[1]))\n",
    "        node_text.append(adjacencies[0])\n",
    "    node_trace.marker.color = node_adjacencies\n",
    "    node_trace.text = node_text\n",
    "    fig = go.Figure(\n",
    "        data=[edge_trace, node_trace, invisible_similarity_trace],\n",
    "        layout=go.Layout(\n",
    "            title=None,\n",
    "            template='plotly_dark',\n",
    "            titlefont_size=20,\n",
    "            showlegend=False,\n",
    "            coloraxis=None,\n",
    "            hovermode='closest',\n",
    "            margin=dict(b=20,l=20,r=20,t=40),\n",
    "            annotations=[\n",
    "                dict(\n",
    "                    text='Word Associations',\n",
    "                    showarrow=False,\n",
    "                    xref=\"paper\", yref=\"paper\",\n",
    "                    x=0.005, y=-0.002 ) \n",
    "            ],\n",
    "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "        )\n",
    "    )\n",
    "    #fig.update_coloraxes(showscale=False)\n",
    "    #fig.update_layout(showscale=False, showlegend=False)\n",
    "\n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc07213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_list = list(american_embassies_w2v_english.wv.index2entity[:10])\n",
    "#countries = ['afghanistan', 'albania', 'algeria', 'angola', 'argentina', 'armenia', 'australia', 'austria', 'azerbaijan', 'bahamas', 'bahrain', 'bangladesh', 'barbados', 'belarus', 'belgium', 'belize', 'benin', 'bolivia', 'bosnia', 'botswana', 'brazil', 'brunei', 'bulgaria', 'burkina', 'burundi', 'cabo', 'cambodia', 'cameroon', 'canada', 'chad', 'chile', 'china', 'colombia', 'comoros', 'congo', 'costa', 'côte', 'croatia', 'cuba', 'cyprus', 'czechia', 'denmark', 'djibouti', 'dominican', 'ecuador', 'egypt', 'elsalvador', 'equatorialguinea', 'eritrea', 'estonia', 'eswatini', 'ethiopia', 'fiji', 'finland', 'france', 'gabon', 'georgia', 'germany', 'ghana', 'greece', 'guatemala', 'guineabissau', 'guinea', 'guyana', 'haiti', 'holysee','vatican','honduras', 'hungary', 'iceland', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', 'northkorea','southkorea', 'kuwait', 'kyrgyzstan', 'lao', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'lithuania', 'luxembourg', 'madagascar', 'malawi', 'malaysia', 'mali', 'malta', 'mauritania', 'mauritius', 'mexico', 'micronesia', 'moldova', 'mongolia', 'montenegro', 'morocco', 'mozambique', 'myanmar', 'nepal', 'netherlands', 'newzealand', 'nicaragua', 'niger', 'nigeria', 'norway', 'oman', 'pakistan', 'palau', 'palestine', 'panama', 'paraguay', 'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', 'russia', 'rwanda', 'samoa', 'saudi', 'saudiarabia','senegal', 'serbia', 'seychelles', 'sierra', 'singapore', 'slovakia', 'slovenia', 'somalia', 'spain', 'srilanka', 'sudan', 'suriname', 'sweden', 'switzerland', 'syrian', 'tajikistan', 'tanzania', 'thailand', 'timor', 'togo', 'trinidad', 'tunisia', 'turkey', 'turkmenistan', 'uganda', 'ukraine', 'uae', 'unitedstates', 'uruguay', 'uzbekistan', 'venezuela', 'vietnam', 'yemen', 'zambia', 'zimbabwe']\n",
    "#word_associations = [word[0] for word in american_embassies_w2v_english.wv.most_similar(['russia'],topn=20)]\n",
    "\n",
    "#word2vec_network(american_embassies_w2v_english, countries, threshhold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa45e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_word_association_network(word, model):\n",
    "    word = word.lower()\n",
    "    word_associations = [word[0] for word in model.wv.most_similar([word],topn=40)]\n",
    "    return word2vec_network(model, word_associations, threshhold=0.7)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f590b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_word_association_network('covid', american_embassies_w2v_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b7c0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_word_association_with_countries(model):\n",
    "    countries = ['afghanistan', 'albania', 'algeria', 'angola', 'argentina', 'armenia', 'australia', 'austria', 'azerbaijan', 'bahamas', 'bahrain', 'bangladesh', 'barbados', 'belarus', 'belgium', 'belize', 'benin', 'bolivia', 'bosnia', 'botswana', 'brazil', 'brunei', 'bulgaria', 'burkina', 'burundi', 'cabo', 'cambodia', 'cameroon', 'canada', 'chad', 'chile', 'china', 'colombia', 'comoros', 'congo', 'costa', 'côte', 'croatia', 'cuba', 'cyprus', 'czechia', 'denmark', 'djibouti', 'dominican', 'ecuador', 'egypt', 'elsalvador', 'equatorialguinea', 'eritrea', 'estonia', 'eswatini', 'ethiopia', 'fiji', 'finland', 'france', 'gabon', 'georgia', 'germany', 'ghana', 'greece', 'guatemala', 'guineabissau', 'guinea', 'guyana', 'haiti', 'holysee','vatican','honduras', 'hungary', 'iceland', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', 'northkorea','southkorea', 'kuwait', 'kyrgyzstan', 'lao', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'lithuania', 'luxembourg', 'madagascar', 'malawi', 'malaysia', 'mali', 'malta', 'mauritania', 'mauritius', 'mexico', 'micronesia', 'moldova', 'mongolia', 'montenegro', 'morocco', 'mozambique', 'myanmar', 'nepal', 'netherlands', 'newzealand', 'nicaragua', 'niger', 'nigeria', 'norway', 'oman', 'pakistan', 'palau', 'palestine', 'panama', 'paraguay', 'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', 'russia', 'rwanda', 'samoa', 'saudi', 'saudiarabia','senegal', 'serbia', 'seychelles', 'sierra', 'singapore', 'slovakia', 'slovenia', 'somalia', 'spain', 'srilanka', 'sudan', 'suriname', 'sweden', 'switzerland', 'syria', 'tajikistan', 'tanzania', 'thailand', 'timor', 'togo', 'trinidad', 'tunisia', 'turkey', 'turkmenistan', 'uganda', 'ukraine', 'uae', 'emirates', 'unitedstates', 'uruguay', 'uzbekistan', 'venezuela', 'vietnam', 'yemen', 'zambia', 'zimbabwe']\n",
    "    return word2vec_network(model, countries, threshhold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89682894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis(x=None, data=None, cov=None):\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n",
    "    x    : vector or matrix of data with, say, p columns.\n",
    "    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
    "    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - np.mean(data)\n",
    "    if not cov:\n",
    "        cov = np.cov(data.values.T)\n",
    "    inv_covmat = sp.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "import scipy as sp\n",
    "countries = ['afghanistan', 'albania', 'algeria', 'angola', 'argentina', 'armenia', 'australia', 'austria', 'azerbaijan', 'bahamas', 'bahrain', 'bangladesh', 'barbados', 'belarus', 'belgium', 'belize', 'benin', 'bolivia', 'bosnia', 'botswana', 'brazil', 'brunei', 'bulgaria', 'burkina', 'burundi', 'cabo', 'cambodia', 'cameroon', 'canada', 'chad', 'chile', 'china', 'colombia', 'comoros', 'congo', 'costa', 'côte', 'croatia', 'cuba', 'cyprus', 'czechia', 'denmark', 'djibouti', 'dominican', 'ecuador', 'egypt', 'elsalvador', 'equatorialguinea', 'eritrea', 'estonia', 'eswatini', 'ethiopia', 'fiji', 'finland', 'france', 'gabon', 'georgia', 'germany', 'ghana', 'greece', 'guatemala', 'guineabissau', 'guinea', 'guyana', 'haiti','vatican','honduras', 'hungary', 'iceland', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', 'northkorea','southkorea', 'kuwait', 'kyrgyzstan', 'lao', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'lithuania', 'luxembourg', 'madagascar', 'malawi', 'malaysia', 'mali', 'malta', 'mauritania', 'mauritius', 'mexico', 'moldova', 'mongolia', 'montenegro', 'morocco', 'mozambique', 'myanmar', 'nepal', 'netherlands', 'newzealand', 'nicaragua', 'niger', 'nigeria', 'norway', 'oman', 'pakistan', 'palau', 'palestine', 'panama', 'paraguay', 'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', 'russia', 'rwanda', 'saudi', 'saudiarabia','senegal', 'serbia', 'seychelles', 'sierra', 'singapore', 'slovakia', 'slovenia', 'somalia', 'spain', 'srilanka', 'sudan', 'suriname', 'sweden', 'switzerland', 'syria', 'tajikistan', 'tanzania', 'thailand', 'togo', 'trinidad', 'tunisia', 'turkey', 'turkmenistan', 'uganda', 'ukraine', 'uae', 'emirates', 'unitedstates', 'uruguay', 'uzbekistan', 'venezuela', 'vietnam', 'yemen', 'zambia', 'zimbabwe']\n",
    "    \n",
    "def get_mahalanobis_outlier_rank(model, category_list):\n",
    "    matrix = []\n",
    "    for item in category_list:\n",
    "        vector = model.wv.get_vector(item)\n",
    "        matrix.append(vector)\n",
    "    matrix = np.vstack(matrix)\n",
    "    vector_df = pd.DataFrame(matrix)\n",
    "    vector_df.index = category_list\n",
    "    columns = list(range(100))\n",
    "    df_x = vector_df[columns]\n",
    "    vector_df['mahalanobis'] = mahalanobis(x=df_x, data=vector_df[columns])\n",
    "    return pd.DataFrame(vector_df['mahalanobis'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5292b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Russia - Contextual Outliers\n",
    "#df = get_mahalanobis_outlier_rank(russian_embassies_w2v_english,countries)#.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff2e8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43ae006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## United States - Contextual Outliers\n",
    "#get_mahalanobis_outlier_rank(american_embassies_w2v_english,countries).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15763327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3601357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(word2vec_word_association_with_countries(american_embassies_w2v_english),key=operator.itemgetter(3),reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f14cb",
   "metadata": {},
   "source": [
    "## Categorical Context Deviation Score? \n",
    "#### What does it tell us? -- The extent to which a token differs in context from ostensibly similar tokens. \n",
    "#### In this case, which countries are referenced in relatively 'atypical' contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8036f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = word2vec_word_association_with_countries(american_embassies_w2v_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc15cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{k: np.round(len(d.keys())/v,3) for k, v in sorted(d.items(), key=lambda item: item[1],reverse=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "cb8f36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BERT NETWORK\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def BERT_network(df,network,threshhold=0.5):\n",
    "    dff = df[df['network']==network]\n",
    "    words, vectors = [], []\n",
    "    for idx in range(len(dff)):\n",
    "        vectors.append(np.array(dff[list(dff.columns)[1:-2]].iloc[idx]))\n",
    "        words.append(df.iloc[idx]['country'])\n",
    "    sims = cosine_similarity(vectors, vectors)       \n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(len(vectors)):\n",
    "            if i<=j:\n",
    "                sims[i, j] = False\n",
    "    indices = np.argwhere(sims > threshhold)\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for index in indices:\n",
    "        G.add_edge(words[index[0]], words[index[1]], weight=sims[index[0],\n",
    "                                                                 index[1]])\n",
    "\n",
    "    weight_values = nx.get_edge_attributes(G,'weight')\n",
    "    positions = nx.spring_layout(G)\n",
    "    nx.set_node_attributes(G,name='position',values=positions)\n",
    "    searches = []\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    weights = []\n",
    "    ave_x, ave_y = [], []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = G.nodes[edge[0]]['position']\n",
    "        x1, y1 = G.nodes[edge[1]]['position']\n",
    "        edge_x.append(x0)\n",
    "        edge_x.append(x1)\n",
    "        edge_x.append(None)\n",
    "        edge_y.append(y0)\n",
    "        edge_y.append(y1)\n",
    "        edge_y.append(None)\n",
    "        ave_x.append(np.mean([x0, x1]))\n",
    "        ave_y.append(np.mean([y0, y1]))\n",
    "        weights.append(f'{edge[0]}, {edge[1]}: {weight_values[(edge[0], edge[1])]}')\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        opacity=0.3,\n",
    "        line=dict(width=2, color='White'),\n",
    "        hoverinfo=None,\n",
    "        mode='lines')\n",
    "    edge_trace.text = weights\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    sizes = []\n",
    "    for node in G.nodes():\n",
    "        x, y = G.nodes[node]['position']\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        if node in searches:\n",
    "            sizes.append(50)\n",
    "        else:\n",
    "            sizes.append(15)\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers+text',\n",
    "        hoverinfo='text',\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(\n",
    "            showscale=False,\n",
    "            line=dict(color='White'),\n",
    "            colorscale='RdBu',\n",
    "            reversescale=False,\n",
    "            color=[],\n",
    "            opacity=0.9,\n",
    "            size=sizes,\n",
    "            colorbar=dict(\n",
    "                thickness=15,\n",
    "                title='Node Connections',\n",
    "                xanchor='left',\n",
    "                titleside='right'\n",
    "            ),\n",
    "            line_width=2\n",
    "        )\n",
    "    )\n",
    "    invisible_similarity_trace = go.Scatter(\n",
    "        x=ave_x, y=ave_y,\n",
    "        mode='markers',\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            color=[],\n",
    "            opacity=0,\n",
    "        )\n",
    "    )\n",
    "    invisible_similarity_trace.text=weights\n",
    "    \n",
    "    node_adjacencies = []\n",
    "    node_text = []\n",
    "    for node, adjacencies in enumerate(G.adjacency()):\n",
    "        node_adjacencies.append(len(adjacencies[1]))\n",
    "        node_text.append(adjacencies[0])\n",
    "    node_trace.marker.color = node_adjacencies\n",
    "    node_trace.text = node_text\n",
    "    fig = go.Figure(\n",
    "        data=[edge_trace, node_trace, invisible_similarity_trace],\n",
    "        layout=go.Layout(\n",
    "            title=None,\n",
    "            template='plotly_dark',\n",
    "            titlefont_size=20,\n",
    "            showlegend=False,\n",
    "            coloraxis=None,\n",
    "            hovermode='closest',\n",
    "            margin=dict(b=20,l=20,r=20,t=40),\n",
    "            annotations=[\n",
    "                dict(\n",
    "                    text='Word Associations',\n",
    "                    showarrow=False,\n",
    "                    xref=\"paper\", yref=\"paper\",\n",
    "                    x=0.005, y=-0.002 ) \n",
    "            ],\n",
    "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "        )\n",
    "    )\n",
    "    #fig.update_coloraxes(showscale=False)\n",
    "    #fig.update_layout(showscale=False, showlegend=False)\n",
    "\n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "fe482477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert[list(bert.columns)[1:-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "bf0cd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT_network(bert,'RUS',threshhold=0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d9e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
